{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "independence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_zx4hOGaqBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "0b9559da-2b8c-4961-b73f-f899b10f47ca"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import ensemble\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import copy\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.over_sampling import SMOTE \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.24)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9HMo56L7Lvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1bc66701-f3e1-4274-c52b-cc5b2ca1dec9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L3utK9q0Bqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !cp -r --recursive \"/content/glove.6B.zip\"     \"/content/drive/My Drive/DataScienceCompetitions/glove_embeddings\"\n",
        "!cp -r --recursive  \"/content/drive/My Drive/DataScienceCompetitions/glove_embeddings\"   \"/content/glove.6B.zip\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "911DXbeW0FQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "38fbcad4-799a-4859-baec-fe0831ca1477"
      },
      "source": [
        "!unzip /content/glove.6B.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTkjSFFsNVa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r --recursive \"/content/drive/My Drive/DataScienceCompetitions/AV_Independence_day/train.csv\"  '/content/train.csv'\n",
        "!cp -r --recursive \"/content/drive/My Drive/DataScienceCompetitions/AV_Independence_day/test.csv\"  '/content/test.csv'\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "test  = pd.read_csv(\"/content/test.csv\")"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsR8_0DXTI_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['combined_text'] = train['TITLE'] + train['ABSTRACT']\n",
        "test['combined_text'] = test['TITLE'] + test['ABSTRACT']"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHye7JCGNf-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c5f79f30-52c6-473d-84ce-ad42c20d5167"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "      <th>combined_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps  P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Rotation Invariance Neural Network  Rotation i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...                                      combined_text\n",
              "0   1  ...  Reconstructing Subject-Specific Effect Maps  P...\n",
              "1   2  ...  Rotation Invariance Neural Network  Rotation i...\n",
              "2   3  ...  Spherical polyharmonics and Poisson kernels fo...\n",
              "3   4  ...  A finite element approximation for the stochas...\n",
              "4   5  ...  Comparative study of Discrete Wavelet Transfor...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWbZAr8UYXLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d1d82d1-d741-4490-8cab-bfff5b2f8e84"
      },
      "source": [
        "print(train.ID.nunique())"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xQTOELaPbtt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "6d2fcb21-8222-4687-f785-4cf78f787b45"
      },
      "source": [
        "print(train.TITLE.nunique())\n",
        "train.TITLE.value_counts()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs                                                                   1\n",
              "Learning a Unified Control Policy for Safe Falling                                                                                           1\n",
              "Testing the Young Neutron Star Scenario with Persistent Radio Emission Associated with FRB 121102                                            1\n",
              "Privacy Preserving Identification Using Sparse Approximation with Ambiguization                                                              1\n",
              "Evaluating Quality of Chatbots and Intelligent Conversational Agents                                                                         1\n",
              "                                                                                                                                            ..\n",
              "Calibrated Fairness in Bandits                                                                                                               1\n",
              "Scikit-Multiflow: A Multi-output Streaming Framework                                                                                         1\n",
              "Interstellar communication. VII. Benchmarking inscribed matter probes                                                                        1\n",
              "Khintchine's Theorem with random fractions                                                                                                   1\n",
              "Isomorphism and Morita equivalence classes for crossed products of irrational rotation algebras by cyclic subgroups of $SL_2(\\mathbb{Z})$    1\n",
              "Name: TITLE, Length: 20972, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1zG4iZ61hM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a0d6dd62-4ebb-4bd1-c80d-d922f1ea77b6"
      },
      "source": [
        "print(train[\"Computer Science\"].nunique())\n",
        "print(train.Physics.nunique())\n",
        "print(train.Mathematics.nunique())\n",
        "print(train.Statistics.nunique())\n",
        "print(train[\"Quantitative Biology\"].nunique())\n",
        "print(train[\"Quantitative Finance\"].nunique())"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJILIjUxMzB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "cd09b801-89c4-4d6b-faed-89048c807943"
      },
      "source": [
        "print(train[\"Computer Science\"].value_counts())\n",
        "print(train[\"Physics\"].value_counts())\n",
        "print(train[\"Mathematics\"].value_counts())\n",
        "print(train[\"Statistics\"].value_counts())\n",
        "print(train[\"Quantitative Biology\"].value_counts())\n",
        "print(train[\"Quantitative Finance\"].value_counts())"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    12378\n",
            "1     8594\n",
            "Name: Computer Science, dtype: int64\n",
            "0    14959\n",
            "1     6013\n",
            "Name: Physics, dtype: int64\n",
            "0    15354\n",
            "1     5618\n",
            "Name: Mathematics, dtype: int64\n",
            "0    15766\n",
            "1     5206\n",
            "Name: Statistics, dtype: int64\n",
            "0    20385\n",
            "1      587\n",
            "Name: Quantitative Biology, dtype: int64\n",
            "0    20723\n",
            "1      249\n",
            "Name: Quantitative Finance, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaqELnONrg-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "e161b7ce-5ac7-4d96-9aea-5f4326ac7b62"
      },
      "source": [
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_train['train_flag'] = 1\n",
        "df_train['combined_text'] = df_train['TITLE'] + df_train['ABSTRACT']\n",
        "print(df_train.shape)\n",
        "\n",
        "\n",
        "df_test = pd.read_csv('/content/test.csv')\n",
        "df_test['combined_text'] = df_test['TITLE'] + df_test['ABSTRACT']\n",
        "df_test['train_flag'] = 0\n",
        "df_test['Computer Science'] =    -1\n",
        "df_test['Physics'] =             -1\n",
        "df_test['Mathematics'] =         -1\n",
        "df_test['Statistics'] =          -1\n",
        "df_test['Quantitative Biology'] =-1\n",
        "df_test['Quantitative Finance'] =-1\n",
        "\n",
        "print(df_test.shape)\n",
        "\n",
        "df_data = pd.concat([df_train,df_test])\n",
        "print(df_data.shape)\n",
        "df_data = df_data.reset_index(drop=True)\n",
        "df_data.head()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20972, 11)\n",
            "(8989, 11)\n",
            "(29961, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "      <th>train_flag</th>\n",
              "      <th>combined_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps  P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Rotation Invariance Neural Network  Rotation i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...                                      combined_text\n",
              "0   1  ...  Reconstructing Subject-Specific Effect Maps  P...\n",
              "1   2  ...  Rotation Invariance Neural Network  Rotation i...\n",
              "2   3  ...  Spherical polyharmonics and Poisson kernels fo...\n",
              "3   4  ...  A finite element approximation for the stochas...\n",
              "4   5  ...  Comparative study of Discrete Wavelet Transfor...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrcFvCbrzDQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "bd691a59-689b-48ca-fb92-0c7260fc71a7"
      },
      "source": [
        "line = df_data[\"combined_text\"].loc[[2]].values[0]\n",
        "print(line)\n",
        "print(str(line).lower().split())"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spherical polyharmonics and Poisson kernels for polyharmonic functions  We introduce and develop the notion of spherical polyharmonics, which are a\n",
            "natural generalisation of spherical harmonics. In particular we study the\n",
            "theory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\n",
            "to construct Poisson kernels for polyharmonic functions on the union of rotated\n",
            "balls. We find the representation of Poisson kernels and zonal polyharmonics in\n",
            "terms of the Gegenbauer polynomials. We show the connection between the\n",
            "classical Poisson kernel for harmonic functions on the ball, Poisson kernels\n",
            "for polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\n",
            "kernel for holomorphic functions on the Lie ball.\n",
            "\n",
            "['spherical', 'polyharmonics', 'and', 'poisson', 'kernels', 'for', 'polyharmonic', 'functions', 'we', 'introduce', 'and', 'develop', 'the', 'notion', 'of', 'spherical', 'polyharmonics,', 'which', 'are', 'a', 'natural', 'generalisation', 'of', 'spherical', 'harmonics.', 'in', 'particular', 'we', 'study', 'the', 'theory', 'of', 'zonal', 'polyharmonics,', 'which', 'allows', 'us,', 'analogously', 'to', 'zonal', 'harmonics,', 'to', 'construct', 'poisson', 'kernels', 'for', 'polyharmonic', 'functions', 'on', 'the', 'union', 'of', 'rotated', 'balls.', 'we', 'find', 'the', 'representation', 'of', 'poisson', 'kernels', 'and', 'zonal', 'polyharmonics', 'in', 'terms', 'of', 'the', 'gegenbauer', 'polynomials.', 'we', 'show', 'the', 'connection', 'between', 'the', 'classical', 'poisson', 'kernel', 'for', 'harmonic', 'functions', 'on', 'the', 'ball,', 'poisson', 'kernels', 'for', 'polyharmonic', 'functions', 'on', 'the', 'union', 'of', 'rotated', 'balls,', 'and', 'the', 'cauchy-hua', 'kernel', 'for', 'holomorphic', 'functions', 'on', 'the', 'lie', 'ball.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2eie03w9Zv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "f0fc8936-a072-4e2d-93f9-3eab3d9e0be9"
      },
      "source": [
        "line = df_data[\"combined_text\"].loc[[5]].values[0]\n",
        "print(line)\n",
        "print(str(line).lower().split())"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On maximizing the fundamental frequency of the complement of an obstacle  Let $\\Omega \\subset \\mathbb{R}^n$ be a bounded domain satisfying a\n",
            "Hayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain\n",
            "referred to as \"obstacle\". We are interested in the behaviour of the first\n",
            "Dirichlet eigenvalue $ \\lambda_1(\\Omega \\setminus (x+D)) $. First, we prove an\n",
            "upper bound on $ \\lambda_1(\\Omega \\setminus (x+D)) $ in terms of the distance\n",
            "of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet\n",
            "ground state $ \\phi_{\\lambda_1} > 0 $ of $ \\Omega $. In short, a direct\n",
            "corollary is that if \\begin{equation} \\mu_\\Omega := \\max_{x}\\lambda_1(\\Omega\n",
            "\\setminus (x+D)) \\end{equation} is large enough in terms of $ \\lambda_1(\\Omega)\n",
            "$, then all maximizer sets $ x+D $ of $ \\mu_\\Omega $ are close to each maximum\n",
            "point $ x_0 $ of $ \\phi_{\\lambda_1} $.\n",
            "Second, we discuss the distribution of $ \\phi_{\\lambda_1(\\Omega)} $ and the\n",
            "possibility to inscribe wavelength balls at a given point in $ \\Omega $.\n",
            "Finally, we specify our observations to convex obstacles $ D $ and show that\n",
            "if $ \\mu_\\Omega $ is sufficiently large with respect to $ \\lambda_1(\\Omega) $,\n",
            "then all maximizers $ x+D $ of $ \\mu_\\Omega $ contain all maximum points $ x_0\n",
            "$ of $ \\phi_{\\lambda_1(\\Omega)} $.\n",
            "\n",
            "['on', 'maximizing', 'the', 'fundamental', 'frequency', 'of', 'the', 'complement', 'of', 'an', 'obstacle', 'let', '$\\\\omega', '\\\\subset', '\\\\mathbb{r}^n$', 'be', 'a', 'bounded', 'domain', 'satisfying', 'a', 'hayman-type', 'asymmetry', 'condition,', 'and', 'let', '$', 'd', '$', 'be', 'an', 'arbitrary', 'bounded', 'domain', 'referred', 'to', 'as', '\"obstacle\".', 'we', 'are', 'interested', 'in', 'the', 'behaviour', 'of', 'the', 'first', 'dirichlet', 'eigenvalue', '$', '\\\\lambda_1(\\\\omega', '\\\\setminus', '(x+d))', '$.', 'first,', 'we', 'prove', 'an', 'upper', 'bound', 'on', '$', '\\\\lambda_1(\\\\omega', '\\\\setminus', '(x+d))', '$', 'in', 'terms', 'of', 'the', 'distance', 'of', 'the', 'set', '$', 'x+d', '$', 'to', 'the', 'set', 'of', 'maximum', 'points', '$', 'x_0', '$', 'of', 'the', 'first', 'dirichlet', 'ground', 'state', '$', '\\\\phi_{\\\\lambda_1}', '>', '0', '$', 'of', '$', '\\\\omega', '$.', 'in', 'short,', 'a', 'direct', 'corollary', 'is', 'that', 'if', '\\\\begin{equation}', '\\\\mu_\\\\omega', ':=', '\\\\max_{x}\\\\lambda_1(\\\\omega', '\\\\setminus', '(x+d))', '\\\\end{equation}', 'is', 'large', 'enough', 'in', 'terms', 'of', '$', '\\\\lambda_1(\\\\omega)', '$,', 'then', 'all', 'maximizer', 'sets', '$', 'x+d', '$', 'of', '$', '\\\\mu_\\\\omega', '$', 'are', 'close', 'to', 'each', 'maximum', 'point', '$', 'x_0', '$', 'of', '$', '\\\\phi_{\\\\lambda_1}', '$.', 'second,', 'we', 'discuss', 'the', 'distribution', 'of', '$', '\\\\phi_{\\\\lambda_1(\\\\omega)}', '$', 'and', 'the', 'possibility', 'to', 'inscribe', 'wavelength', 'balls', 'at', 'a', 'given', 'point', 'in', '$', '\\\\omega', '$.', 'finally,', 'we', 'specify', 'our', 'observations', 'to', 'convex', 'obstacles', '$', 'd', '$', 'and', 'show', 'that', 'if', '$', '\\\\mu_\\\\omega', '$', 'is', 'sufficiently', 'large', 'with', 'respect', 'to', '$', '\\\\lambda_1(\\\\omega)', '$,', 'then', 'all', 'maximizers', '$', 'x+d', '$', 'of', '$', '\\\\mu_\\\\omega', '$', 'contain', 'all', 'maximum', 'points', '$', 'x_0', '$', 'of', '$', '\\\\phi_{\\\\lambda_1(\\\\omega)}', '$.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGDpHnJAzCTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "fbd57b5b-6770-4406-e443-1467f673572a"
      },
      "source": [
        "line = df_data[\"combined_text\"].loc[[5]].values[0]\n",
        "print(line)\n",
        "line = line.replace('\"', '')\n",
        "line = line.replace('\\'', ' ')\n",
        "line = line.replace(\".\",' ')\n",
        "line = line.replace(\"#\",' ')\n",
        "line = line.replace(\"&\",' and ')\n",
        "line = line.replace(\"!\",' ')\n",
        "line = line.replace(\";\",' ')\n",
        "line = line.replace(\":\",' ')\n",
        "line = line.replace(\",\",' ')\n",
        "line = line.replace(\"~\",' ')\n",
        "line = line.replace(\")\",' ')\n",
        "line = line.replace(\"(\",' ')\n",
        "line = line.replace(\"}\",' ')\n",
        "line = line.replace(\"{\",' ')\n",
        "line = line.replace(\"[\",' ')\n",
        "line = line.replace(\"]\",' ')\n",
        "\n",
        "print(line)\n",
        "sentence = str(line).lower().split()\n",
        "print(sentence)\n",
        "\n",
        "for ind,word in enumerate(sentence):\n",
        "    if '\\\\' in word or \"/\" in word  or \"$\" in word  or \"_\" in word or word[0]==\"-\" or \"*\" in word or \"+\" in word or \"@\" in word  :\n",
        "        sentence[ind] = \"CONST\"\n",
        "print(sentence)\n",
        "\n",
        "while 1:\n",
        "    if \"CONST\" in sentence:\n",
        "        sentence.remove(\"CONST\")\n",
        "    else:\n",
        "        break\n",
        "print(sentence)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On maximizing the fundamental frequency of the complement of an obstacle  Let $\\Omega \\subset \\mathbb{R}^n$ be a bounded domain satisfying a\n",
            "Hayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain\n",
            "referred to as \"obstacle\". We are interested in the behaviour of the first\n",
            "Dirichlet eigenvalue $ \\lambda_1(\\Omega \\setminus (x+D)) $. First, we prove an\n",
            "upper bound on $ \\lambda_1(\\Omega \\setminus (x+D)) $ in terms of the distance\n",
            "of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet\n",
            "ground state $ \\phi_{\\lambda_1} > 0 $ of $ \\Omega $. In short, a direct\n",
            "corollary is that if \\begin{equation} \\mu_\\Omega := \\max_{x}\\lambda_1(\\Omega\n",
            "\\setminus (x+D)) \\end{equation} is large enough in terms of $ \\lambda_1(\\Omega)\n",
            "$, then all maximizer sets $ x+D $ of $ \\mu_\\Omega $ are close to each maximum\n",
            "point $ x_0 $ of $ \\phi_{\\lambda_1} $.\n",
            "Second, we discuss the distribution of $ \\phi_{\\lambda_1(\\Omega)} $ and the\n",
            "possibility to inscribe wavelength balls at a given point in $ \\Omega $.\n",
            "Finally, we specify our observations to convex obstacles $ D $ and show that\n",
            "if $ \\mu_\\Omega $ is sufficiently large with respect to $ \\lambda_1(\\Omega) $,\n",
            "then all maximizers $ x+D $ of $ \\mu_\\Omega $ contain all maximum points $ x_0\n",
            "$ of $ \\phi_{\\lambda_1(\\Omega)} $.\n",
            "\n",
            "On maximizing the fundamental frequency of the complement of an obstacle  Let $\\Omega \\subset \\mathbb R ^n$ be a bounded domain satisfying a\n",
            "Hayman-type asymmetry condition  and let $ D $ be an arbitrary bounded domain\n",
            "referred to as obstacle  We are interested in the behaviour of the first\n",
            "Dirichlet eigenvalue $ \\lambda_1 \\Omega \\setminus  x+D   $  First  we prove an\n",
            "upper bound on $ \\lambda_1 \\Omega \\setminus  x+D   $ in terms of the distance\n",
            "of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet\n",
            "ground state $ \\phi_ \\lambda_1  > 0 $ of $ \\Omega $  In short  a direct\n",
            "corollary is that if \\begin equation  \\mu_\\Omega  = \\max_ x \\lambda_1 \\Omega\n",
            "\\setminus  x+D   \\end equation  is large enough in terms of $ \\lambda_1 \\Omega \n",
            "$  then all maximizer sets $ x+D $ of $ \\mu_\\Omega $ are close to each maximum\n",
            "point $ x_0 $ of $ \\phi_ \\lambda_1  $ \n",
            "Second  we discuss the distribution of $ \\phi_ \\lambda_1 \\Omega   $ and the\n",
            "possibility to inscribe wavelength balls at a given point in $ \\Omega $ \n",
            "Finally  we specify our observations to convex obstacles $ D $ and show that\n",
            "if $ \\mu_\\Omega $ is sufficiently large with respect to $ \\lambda_1 \\Omega  $ \n",
            "then all maximizers $ x+D $ of $ \\mu_\\Omega $ contain all maximum points $ x_0\n",
            "$ of $ \\phi_ \\lambda_1 \\Omega   $ \n",
            "\n",
            "['on', 'maximizing', 'the', 'fundamental', 'frequency', 'of', 'the', 'complement', 'of', 'an', 'obstacle', 'let', '$\\\\omega', '\\\\subset', '\\\\mathbb', 'r', '^n$', 'be', 'a', 'bounded', 'domain', 'satisfying', 'a', 'hayman-type', 'asymmetry', 'condition', 'and', 'let', '$', 'd', '$', 'be', 'an', 'arbitrary', 'bounded', 'domain', 'referred', 'to', 'as', 'obstacle', 'we', 'are', 'interested', 'in', 'the', 'behaviour', 'of', 'the', 'first', 'dirichlet', 'eigenvalue', '$', '\\\\lambda_1', '\\\\omega', '\\\\setminus', 'x+d', '$', 'first', 'we', 'prove', 'an', 'upper', 'bound', 'on', '$', '\\\\lambda_1', '\\\\omega', '\\\\setminus', 'x+d', '$', 'in', 'terms', 'of', 'the', 'distance', 'of', 'the', 'set', '$', 'x+d', '$', 'to', 'the', 'set', 'of', 'maximum', 'points', '$', 'x_0', '$', 'of', 'the', 'first', 'dirichlet', 'ground', 'state', '$', '\\\\phi_', '\\\\lambda_1', '>', '0', '$', 'of', '$', '\\\\omega', '$', 'in', 'short', 'a', 'direct', 'corollary', 'is', 'that', 'if', '\\\\begin', 'equation', '\\\\mu_\\\\omega', '=', '\\\\max_', 'x', '\\\\lambda_1', '\\\\omega', '\\\\setminus', 'x+d', '\\\\end', 'equation', 'is', 'large', 'enough', 'in', 'terms', 'of', '$', '\\\\lambda_1', '\\\\omega', '$', 'then', 'all', 'maximizer', 'sets', '$', 'x+d', '$', 'of', '$', '\\\\mu_\\\\omega', '$', 'are', 'close', 'to', 'each', 'maximum', 'point', '$', 'x_0', '$', 'of', '$', '\\\\phi_', '\\\\lambda_1', '$', 'second', 'we', 'discuss', 'the', 'distribution', 'of', '$', '\\\\phi_', '\\\\lambda_1', '\\\\omega', '$', 'and', 'the', 'possibility', 'to', 'inscribe', 'wavelength', 'balls', 'at', 'a', 'given', 'point', 'in', '$', '\\\\omega', '$', 'finally', 'we', 'specify', 'our', 'observations', 'to', 'convex', 'obstacles', '$', 'd', '$', 'and', 'show', 'that', 'if', '$', '\\\\mu_\\\\omega', '$', 'is', 'sufficiently', 'large', 'with', 'respect', 'to', '$', '\\\\lambda_1', '\\\\omega', '$', 'then', 'all', 'maximizers', '$', 'x+d', '$', 'of', '$', '\\\\mu_\\\\omega', '$', 'contain', 'all', 'maximum', 'points', '$', 'x_0', '$', 'of', '$', '\\\\phi_', '\\\\lambda_1', '\\\\omega', '$']\n",
            "['on', 'maximizing', 'the', 'fundamental', 'frequency', 'of', 'the', 'complement', 'of', 'an', 'obstacle', 'let', 'CONST', 'CONST', 'CONST', 'r', 'CONST', 'be', 'a', 'bounded', 'domain', 'satisfying', 'a', 'hayman-type', 'asymmetry', 'condition', 'and', 'let', 'CONST', 'd', 'CONST', 'be', 'an', 'arbitrary', 'bounded', 'domain', 'referred', 'to', 'as', 'obstacle', 'we', 'are', 'interested', 'in', 'the', 'behaviour', 'of', 'the', 'first', 'dirichlet', 'eigenvalue', 'CONST', 'CONST', 'CONST', 'CONST', 'CONST', 'CONST', 'first', 'we', 'prove', 'an', 'upper', 'bound', 'on', 'CONST', 'CONST', 'CONST', 'CONST', 'CONST', 'CONST', 'in', 'terms', 'of', 'the', 'distance', 'of', 'the', 'set', 'CONST', 'CONST', 'CONST', 'to', 'the', 'set', 'of', 'maximum', 'points', 'CONST', 'CONST', 'CONST', 'of', 'the', 'first', 'dirichlet', 'ground', 'state', 'CONST', 'CONST', 'CONST', '>', '0', 'CONST', 'of', 'CONST', 'CONST', 'CONST', 'in', 'short', 'a', 'direct', 'corollary', 'is', 'that', 'if', 'CONST', 'equation', 'CONST', '=', 'CONST', 'x', 'CONST', 'CONST', 'CONST', 'CONST', 'CONST', 'equation', 'is', 'large', 'enough', 'in', 'terms', 'of', 'CONST', 'CONST', 'CONST', 'CONST', 'then', 'all', 'maximizer', 'sets', 'CONST', 'CONST', 'CONST', 'of', 'CONST', 'CONST', 'CONST', 'are', 'close', 'to', 'each', 'maximum', 'point', 'CONST', 'CONST', 'CONST', 'of', 'CONST', 'CONST', 'CONST', 'CONST', 'second', 'we', 'discuss', 'the', 'distribution', 'of', 'CONST', 'CONST', 'CONST', 'CONST', 'CONST', 'and', 'the', 'possibility', 'to', 'inscribe', 'wavelength', 'balls', 'at', 'a', 'given', 'point', 'in', 'CONST', 'CONST', 'CONST', 'finally', 'we', 'specify', 'our', 'observations', 'to', 'convex', 'obstacles', 'CONST', 'd', 'CONST', 'and', 'show', 'that', 'if', 'CONST', 'CONST', 'CONST', 'is', 'sufficiently', 'large', 'with', 'respect', 'to', 'CONST', 'CONST', 'CONST', 'CONST', 'then', 'all', 'maximizers', 'CONST', 'CONST', 'CONST', 'of', 'CONST', 'CONST', 'CONST', 'contain', 'all', 'maximum', 'points', 'CONST', 'CONST', 'CONST', 'of', 'CONST', 'CONST', 'CONST', 'CONST', 'CONST']\n",
            "['on', 'maximizing', 'the', 'fundamental', 'frequency', 'of', 'the', 'complement', 'of', 'an', 'obstacle', 'let', 'r', 'be', 'a', 'bounded', 'domain', 'satisfying', 'a', 'hayman-type', 'asymmetry', 'condition', 'and', 'let', 'd', 'be', 'an', 'arbitrary', 'bounded', 'domain', 'referred', 'to', 'as', 'obstacle', 'we', 'are', 'interested', 'in', 'the', 'behaviour', 'of', 'the', 'first', 'dirichlet', 'eigenvalue', 'first', 'we', 'prove', 'an', 'upper', 'bound', 'on', 'in', 'terms', 'of', 'the', 'distance', 'of', 'the', 'set', 'to', 'the', 'set', 'of', 'maximum', 'points', 'of', 'the', 'first', 'dirichlet', 'ground', 'state', '>', '0', 'of', 'in', 'short', 'a', 'direct', 'corollary', 'is', 'that', 'if', 'equation', '=', 'x', 'equation', 'is', 'large', 'enough', 'in', 'terms', 'of', 'then', 'all', 'maximizer', 'sets', 'of', 'are', 'close', 'to', 'each', 'maximum', 'point', 'of', 'second', 'we', 'discuss', 'the', 'distribution', 'of', 'and', 'the', 'possibility', 'to', 'inscribe', 'wavelength', 'balls', 'at', 'a', 'given', 'point', 'in', 'finally', 'we', 'specify', 'our', 'observations', 'to', 'convex', 'obstacles', 'd', 'and', 'show', 'that', 'if', 'is', 'sufficiently', 'large', 'with', 'respect', 'to', 'then', 'all', 'maximizers', 'of', 'contain', 'all', 'maximum', 'points', 'of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LqfFkRxZgMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "78d0f1bf-8fae-4e9c-fc8e-1e5b223664bf"
      },
      "source": [
        "df_data.info()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 29961 entries, 0 to 29960\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   ID                    29961 non-null  int64 \n",
            " 1   TITLE                 29961 non-null  object\n",
            " 2   ABSTRACT              29961 non-null  object\n",
            " 3   Computer Science      29961 non-null  int64 \n",
            " 4   Physics               29961 non-null  int64 \n",
            " 5   Mathematics           29961 non-null  int64 \n",
            " 6   Statistics            29961 non-null  int64 \n",
            " 7   Quantitative Biology  29961 non-null  int64 \n",
            " 8   Quantitative Finance  29961 non-null  int64 \n",
            " 9   train_flag            29961 non-null  int64 \n",
            " 10  combined_text         29961 non-null  object\n",
            "dtypes: int64(8), object(3)\n",
            "memory usage: 2.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRZgyPkQ2qKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuXs2vK_2SWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "b6de47cd-1743-462d-b43c-8f17faef6f35"
      },
      "source": [
        "# here u will get an array with various strings seperated by space\n",
        "tkr = RegexpTokenizer('[a-zA-Z]+')\n",
        "abs_list = []\n",
        "computer = []\n",
        "physics = []\n",
        "maths = []\n",
        "stats = []\n",
        "bio = []\n",
        "finance = []\n",
        "\n",
        "abs_test = []\n",
        "colID = []\n",
        "\n",
        "\n",
        "for i in range(len(df_data)):\n",
        "    line = df_data[\"combined_text\"][i]\n",
        "\n",
        "    line = line.replace('\"', '')\n",
        "    line = line.replace('\\'', ' ')\n",
        "    line = line.replace(\".\",' ')\n",
        "    line = line.replace(\"#\",' ')\n",
        "    line = line.replace(\"&\",' and ')\n",
        "    line = line.replace(\"!\",' ')\n",
        "    line = line.replace(\";\",' ')\n",
        "    line = line.replace(\":\",' ')\n",
        "    line = line.replace(\",\",' ')\n",
        "    line = line.replace(\"~\",' ')\n",
        "    line = line.replace(\")\",' ')\n",
        "    line = line.replace(\"(\",' ')\n",
        "    line = line.replace(\"}\",' ')\n",
        "    line = line.replace(\"{\",' ')\n",
        "    line = line.replace(\"[\",' ')\n",
        "    line = line.replace(\"]\",' ')\n",
        "\n",
        "    sentence = str(line).lower().split()\n",
        "\n",
        "    for ind,word in enumerate(sentence):\n",
        "        if '\\\\' in word or \"/\" in word  or \"$\" in word  or \"_\" in word or word[0]==\"-\" or \"*\" in word or \"+\" in word or \"@\" in word  :\n",
        "            sentence[ind] = \"CONST\"\n",
        "\n",
        "    while 1:\n",
        "        if \"CONST\" in sentence:\n",
        "            sentence.remove(\"CONST\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    sentence = tkr.tokenize(str(sentence))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if df_data[\"train_flag\"][i] == 1:\n",
        "        abs_list.append(sentence)\n",
        "\n",
        "        computer.append(df_data[\"Computer Science\"][i])\n",
        "        physics.append(df_data[\"Physics\"][i])\n",
        "        maths.append(df_data[\"Mathematics\"][i])\n",
        "        stats.append(df_data[\"Statistics\"][i])\n",
        "        bio.append(df_data[\"Quantitative Biology\"][i])\n",
        "        finance.append(df_data[\"Quantitative Finance\"][i])\n",
        "    else:\n",
        "        abs_test.append(sentence)\n",
        "        colID.append(df_data[\"ID\"][i])\n",
        "        \n",
        "\n",
        "\n",
        "print(abs_list[3])\n",
        "print(len(abs_list))\n",
        "print(abs_test[3])\n",
        "print(len(abs_test))\n",
        "\n",
        "print(computer[:10])\n",
        "print(physics[:10])\n",
        "print(maths[:10])\n",
        "print(stats[:10])\n",
        "print(bio[:10])\n",
        "print(finance[:10])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'finite', 'element', 'approximation', 'for', 'the', 'stochastic', 'maxwell', 'landau', 'lifshitz', 'gilbert', 'system', 'the', 'stochastic', 'landau', 'lifshitz', 'gilbert', 'llg', 'equation', 'coupled', 'with', 'the', 'maxwell', 'equations', 'the', 'so', 'called', 'stochastic', 'mllg', 'system', 'describes', 'the', 'creation', 'of', 'domain', 'walls', 'and', 'vortices', 'fundamental', 'objects', 'for', 'the', 'novel', 'nanostructured', 'magnetic', 'memories', 'we', 'first', 'reformulate', 'the', 'stochastic', 'llg', 'equation', 'into', 'an', 'equation', 'with', 'time', 'differentiable', 'solutions', 'we', 'then', 'propose', 'a', 'convergent', 'scheme', 'to', 'approximate', 'the', 'solutions', 'of', 'the', 'reformulated', 'system', 'as', 'a', 'consequence', 'we', 'prove', 'convergence', 'of', 'the', 'approximate', 'solutions', 'with', 'no', 'or', 'minor', 'conditions', 'on', 'time', 'and', 'space', 'steps', 'depending', 'on', 'the', 'value', 'of', 'hence', 'we', 'prove', 'the', 'existence', 'of', 'weak', 'martingale', 'solutions', 'of', 'the', 'stochastic', 'mllg', 'system', 'numerical', 'results', 'are', 'presented', 'to', 'show', 'applicability', 'of', 'the', 'method']\n",
            "20972\n",
            "['the', 'survey', 'the', 'inner', 'disk', 'intermediate', 'age', 'open', 'cluster', 'ngc', 'milky', 'way', 'open', 'clusters', 'are', 'very', 'diverse', 'in', 'terms', 'of', 'age', 'chemical', 'composition', 'and', 'kinematic', 'properties', 'intermediate', 'age', 'and', 'old', 'open', 'clusters', 'are', 'less', 'common', 'and', 'it', 'is', 'even', 'harder', 'to', 'find', 'them', 'inside', 'the', 'solar', 'galactocentric', 'radius', 'due', 'to', 'the', 'high', 'mortality', 'rate', 'and', 'strong', 'extinction', 'inside', 'this', 'region', 'ngc', 'is', 'one', 'of', 'the', 'inner', 'disk', 'open', 'clusters', 'iocs', 'observed', 'by', 'the', 'survey', 'ges', 'this', 'cluster', 'is', 'an', 'important', 'target', 'for', 'calibrating', 'the', 'abundances', 'derived', 'in', 'the', 'survey', 'due', 'to', 'the', 'kinematic', 'and', 'chemical', 'homogeneity', 'of', 'the', 'members', 'in', 'open', 'clusters', 'using', 'the', 'measurements', 'from', 'internal', 'data', 'release', 'idr', 'we', 'identify', 'main', 'sequence', 'dwarfs', 'as', 'cluster', 'members', 'from', 'the', 'giraffe', 'target', 'list', 'and', 'eight', 'giants', 'as', 'cluster', 'members', 'from', 'the', 'uves', 'target', 'list', 'the', 'dwarf', 'cluster', 'members', 'have', 'a', 'median', 'radial', 'velocity', 'of', 'km', 'while', 'the', 'giant', 'cluster', 'members', 'have', 'a', 'median', 'radial', 'velocity', 'of', 'km', 'and', 'a', 'median', 'of', 'dex', 'the', 'color', 'magnitude', 'diagram', 'of', 'these', 'cluster', 'members', 'suggests', 'an', 'age', 'of', 'gyr', 'with', 'm', 'm', 'and', 'b', 'v', 'we', 'perform', 'the', 'first', 'detailed', 'chemical', 'abundance', 'analysis', 'of', 'ngc', 'including', 'elemental', 'species', 'to', 'gain', 'a', 'more', 'general', 'picture', 'about', 'iocs', 'the', 'measurements', 'of', 'ngc', 'are', 'compared', 'with', 'those', 'of', 'other', 'iocs', 'previously', 'studied', 'by', 'ges', 'that', 'is', 'ngc', 'trumpler', 'ngc', 'and', 'berkeley', 'ngc', 'shows', 'similar', 'c', 'n', 'na', 'and', 'al', 'abundances', 'as', 'other', 'iocs', 'these', 'elements', 'are', 'compared', 'with', 'nucleosynthetic', 'models', 'as', 'a', 'function', 'of', 'cluster', 'turn', 'off', 'mass', 'the', 'iron', 'peak', 'and', 'neutron', 'capture', 'elements', 'are', 'also', 'explored', 'in', 'a', 'self', 'consistent', 'way']\n",
            "8989\n",
            "[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
            "[0, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbys0nTp4YWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# aa = df_data[\"ABSTRACT\"][0]\n",
        "# print(aa)\n",
        "# print(len(aa.split()) , aa.split())\n",
        "# print(len(aa.lower().split())  ,aa.lower().split())\n",
        "# tkr = RegexpTokenizer('[a-zA-Z]+')\n",
        "# zz = str(aa.lower().split())\n",
        "# print(len(zz) , zz)\n",
        "# print(len(tkr.tokenize(zz)) , tkr.tokenize(zz))\n",
        "# tkr.tokenize(\"[dg fb][n]\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27pLFsSH3FIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "8f68eeea-883b-4a09-e3e6-04972a8b1127"
      },
      "source": [
        "# here, u will get a sequence for each word in all strings\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(abs_list)\n",
        "sequenceTrain = tokenizer.texts_to_sequences(abs_list)\n",
        "print(sequenceTrain[0])\n",
        "print(len(sequenceTrain))\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(abs_test)\n",
        "sequenceTest = tokenizer.texts_to_sequences(abs_test)\n",
        "print(sequenceTest[0])\n",
        "print(len(sequenceTest))\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4451, 1155, 326, 214, 700, 1029, 42, 842, 1155, 326, 284, 69, 1362, 1537, 294, 8309, 6, 4860, 22, 121, 4, 1155, 49, 22, 284, 21, 19, 805, 26, 34, 866, 342, 112, 108, 33122, 518, 495, 8, 1, 1155, 3, 151, 112, 108, 1283, 518, 214, 12, 103, 520, 727, 1720, 18, 1, 1155, 49, 22, 116, 342, 284, 9, 786, 62, 151, 284, 20, 21, 19, 62, 5, 216, 1155, 326, 214, 700, 9, 3743, 62, 554, 194, 42, 321, 1433, 1090, 4374, 2108, 2, 7883, 2207, 6455, 6, 13, 675, 7, 81, 4, 795, 38, 2111, 12663, 5, 397, 1155, 326, 4374, 2, 1029, 329, 209, 3, 6, 171, 561, 1281, 12663, 678, 1376, 5, 717, 245, 181, 5, 352, 224, 277, 11, 27, 4, 146, 221, 2, 289, 5, 943, 1281, 1, 60, 38, 9, 4, 13439, 144, 53, 10, 21, 19, 62, 11, 65, 561, 1281, 6, 4, 3509, 1357, 112, 108, 218, 79, 12, 518, 495, 795, 9, 2829, 16, 4, 433, 4, 5066, 40, 11, 4, 515, 23, 545, 141, 15, 1030, 18, 129, 22, 6, 4, 958, 326, 2251, 247, 511, 9, 690, 12, 8519, 391, 22, 3, 22, 18, 1, 5067, 49, 1537, 4860, 7416, 11059, 1452, 33, 12, 792, 22, 122, 10, 27, 12663, 1092, 319, 188, 197, 233, 5, 27, 42, 621, 36, 11, 2019, 2696, 1377, 12, 1, 11059, 334, 30, 10, 12663, 21, 39, 397, 487, 43, 1155, 326, 4374, 6, 5472, 2669, 22, 3, 63, 617, 5823, 2, 5067, 49, 1537, 2058, 37, 16, 1, 3554, 4756, 64, 6166, 1396, 3, 26044, 1013, 866, 229, 1974, 312, 12, 1, 2120, 11059, 334, 30, 1027, 12, 188, 1974, 69, 12663, 9, 62]\n",
            "20972\n",
            "[608, 217, 2044, 673, 6, 1583, 1358, 170, 1428, 7, 76, 149, 15731, 2, 1, 1583, 1358, 624, 23, 4, 679, 170, 1428, 23, 8, 2485, 22, 7, 30, 10, 624, 21, 19, 13421, 231, 2, 1, 170, 13, 1004, 394, 47, 1278, 215, 1, 184, 2, 1, 170, 15, 433, 2044, 673, 184, 6, 169, 13, 3427, 1, 907, 2, 13, 574, 5, 74, 2154, 325, 2, 1, 1428, 432, 851, 56, 845, 5, 1130, 8353, 5184, 2435, 4797, 16, 1657, 213, 6, 296, 89, 1, 8761, 2, 1, 1950, 170, 646, 6, 1728, 5, 4, 47, 751, 748, 2039, 1879, 52, 11, 4220, 102]\n",
            "8989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STz7MjSc749q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "96083491-0a56-47fe-d30f-4d706fbc3f57"
      },
      "source": [
        "from collections import defaultdict\n",
        "ma = 0\n",
        "st = \"\"\n",
        "count = defaultdict(int)\n",
        "for i in sequenceTrain:\n",
        "    count[len(i)] = count[len(i)] + 1\n",
        "    if ma < len(i):\n",
        "        ma = len(i)\n",
        "        st = i\n",
        "\n",
        "print(ma)\n",
        "print(st)\n",
        "print(count)\n",
        "\n",
        "# ma = 0\n",
        "# st = \"\"\n",
        "# count = defaultdict(int)\n",
        "# for i in sequenceTest:\n",
        "#     count[len(i)] = count[len(i)] + 1\n",
        "#     if ma < len(i):\n",
        "#         ma = len(i)\n",
        "#         st = i\n",
        "\n",
        "# print(ma)\n",
        "# print(st)\n",
        "# print(count)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "458\n",
            "[17, 2209, 5, 392, 22, 57, 498, 3, 669, 1164, 8, 22, 3550, 392, 22, 57, 11489, 9, 4, 203, 3, 446, 1661, 25926, 920, 4, 90, 2, 51, 392, 3, 721, 777, 5, 1960, 748, 158, 8, 1682, 157, 22, 13, 35, 9, 4, 3598, 2209, 128, 4, 564, 1310, 1912, 5, 982, 498, 3, 669, 1164, 2, 11489, 8, 63, 2421, 2209, 3, 3132, 392, 22, 57, 11489, 9, 4, 203, 25926, 10, 3409, 18, 238, 903, 6, 286, 872, 1000, 3, 270, 605, 336, 1, 9791, 2977, 2, 1, 4228, 798, 50, 21, 1840, 1627, 721, 209, 8, 22, 57, 2017, 859, 6, 1, 1178, 11489, 6606, 4872, 16, 4, 25926, 11, 1, 8729, 903, 2, 27291, 625, 570, 3, 48978, 3, 18770, 6, 2469, 1959, 3, 168, 19503, 6, 4, 6256, 35, 6, 18770, 11489, 9, 1725, 1132, 14, 1, 1222, 10, 1000, 3, 605, 113, 4, 1243, 46, 5, 1960, 308, 2083, 3, 2987, 1071, 79, 220, 1, 91, 2, 22, 2331, 108, 143, 48979, 11489, 1376, 26, 920, 71, 9811, 962, 262, 3, 1806, 56, 5, 1960, 505, 3, 1532, 1, 157, 392, 3, 721, 291, 509, 22, 10, 15, 321, 1539, 16, 154, 1842, 6, 1541, 36, 70, 137, 440, 377, 336, 1, 1334, 564, 730, 4, 2662, 48980, 45, 77, 805, 5, 113, 308, 3, 32848, 22, 291, 3, 98, 8, 11489, 10, 15, 1748, 1056, 3, 333, 3, 1474, 5, 84, 128, 232, 3564, 37, 16, 1, 48981, 2096, 3, 2927, 14264, 625, 570, 3, 58, 213, 473, 1051, 48982, 625, 570, 4, 798, 28, 9, 626, 1950, 2408, 11489, 1748, 287, 4, 90, 2, 8330, 3, 32848, 777, 10, 21, 19, 62, 6, 879, 36, 2511, 5, 97, 22, 3062, 777, 1, 48983, 11489, 45, 327, 163, 2705, 6, 238, 1476, 3, 230, 26338, 166, 1748, 1169, 4, 66, 613, 2, 56, 1289, 14, 392, 3, 721, 209, 920, 4, 548, 2275, 2, 85, 32, 194, 209, 9, 959, 1, 4325, 2, 13, 8769, 718, 110, 131, 2, 280, 1373, 12, 1, 804, 982, 3, 232, 2225, 10, 206, 2352, 16, 1, 6235, 2, 13, 35, 1, 290, 9, 1828, 5, 19, 4, 48984, 90, 2, 292, 3435, 11, 4, 889, 2, 379, 36, 854, 43, 280, 13, 379, 21, 19, 523, 14, 1, 440, 6, 1, 3046, 86, 108, 143, 1, 1541, 440, 69, 1, 22, 15, 1273, 6, 213, 106, 36, 2596, 16, 17, 1264, 440, 31614, 14, 4, 1942, 379, 169, 1, 48985, 2, 1, 440, 12, 1, 22, 9, 1213, 121, 16, 17, 290, 36, 2742, 14, 1, 230, 28, 9, 110, 207, 5, 11458, 10, 1, 1055, 2, 1, 440, 145, 19, 328, 5, 1227, 1046, 392, 3, 721, 158, 2, 1, 22]\n",
            "defaultdict(<class 'int'>, {285: 24, 82: 65, 108: 114, 123: 143, 146: 144, 149: 127, 113: 122, 136: 125, 141: 136, 165: 143, 270: 40, 227: 74, 151: 139, 139: 119, 144: 118, 49: 35, 268: 34, 59: 41, 212: 102, 292: 22, 245: 62, 104: 91, 125: 113, 147: 115, 64: 43, 213: 91, 74: 72, 124: 125, 122: 119, 67: 55, 187: 98, 111: 105, 164: 142, 211: 89, 145: 139, 193: 100, 83: 69, 191: 101, 114: 117, 192: 100, 189: 123, 101: 93, 277: 33, 215: 89, 93: 76, 177: 125, 209: 87, 183: 108, 217: 74, 202: 88, 175: 133, 199: 101, 131: 122, 41: 26, 91: 67, 107: 106, 179: 101, 25: 13, 106: 87, 188: 87, 66: 48, 132: 131, 153: 161, 273: 39, 152: 141, 184: 108, 97: 68, 178: 110, 148: 131, 142: 135, 86: 96, 172: 131, 185: 96, 194: 84, 87: 77, 138: 112, 157: 150, 156: 147, 159: 140, 162: 141, 295: 18, 89: 71, 216: 96, 135: 124, 140: 137, 88: 65, 128: 111, 143: 138, 154: 138, 198: 95, 150: 131, 280: 27, 233: 60, 90: 80, 120: 128, 166: 125, 221: 83, 52: 34, 253: 49, 180: 107, 121: 124, 228: 66, 60: 49, 116: 121, 242: 50, 95: 90, 117: 119, 254: 43, 69: 52, 170: 125, 110: 106, 96: 83, 163: 108, 99: 101, 243: 48, 56: 34, 256: 55, 266: 35, 224: 59, 169: 129, 272: 35, 33: 15, 158: 119, 279: 32, 226: 62, 303: 22, 305: 21, 238: 63, 75: 71, 205: 100, 204: 111, 70: 61, 287: 30, 72: 56, 171: 130, 265: 42, 173: 111, 301: 22, 261: 38, 195: 100, 259: 41, 206: 109, 225: 72, 307: 9, 73: 60, 126: 111, 48: 44, 137: 112, 240: 38, 45: 31, 282: 31, 160: 135, 85: 79, 119: 132, 161: 136, 263: 45, 223: 50, 234: 67, 105: 104, 112: 118, 275: 25, 109: 108, 186: 102, 218: 93, 214: 85, 296: 23, 63: 48, 203: 78, 71: 53, 222: 75, 100: 97, 239: 58, 251: 30, 207: 78, 92: 79, 262: 48, 196: 92, 181: 104, 274: 37, 98: 85, 201: 89, 297: 18, 115: 89, 133: 116, 310: 9, 55: 42, 54: 35, 65: 51, 260: 39, 31: 21, 197: 98, 155: 130, 176: 124, 252: 55, 210: 100, 102: 87, 182: 101, 286: 31, 288: 24, 94: 91, 231: 59, 167: 125, 257: 46, 321: 5, 127: 106, 271: 42, 190: 120, 35: 30, 22: 6, 174: 119, 61: 53, 289: 29, 168: 118, 43: 26, 248: 42, 250: 54, 237: 62, 79: 63, 249: 57, 247: 45, 118: 131, 46: 37, 28: 16, 244: 50, 81: 77, 235: 50, 20: 7, 309: 11, 68: 60, 34: 17, 44: 22, 103: 95, 129: 116, 241: 46, 230: 65, 219: 79, 134: 118, 278: 30, 284: 31, 232: 62, 264: 48, 290: 27, 130: 119, 313: 12, 229: 62, 36: 15, 291: 18, 281: 28, 57: 44, 47: 33, 302: 11, 220: 60, 208: 89, 78: 67, 306: 8, 51: 29, 299: 17, 42: 26, 255: 47, 298: 15, 84: 84, 53: 22, 77: 67, 236: 59, 304: 12, 276: 31, 200: 92, 300: 18, 29: 11, 80: 64, 269: 47, 39: 18, 322: 4, 62: 50, 258: 47, 76: 65, 267: 36, 27: 11, 21: 5, 24: 13, 26: 10, 40: 19, 294: 25, 30: 13, 246: 49, 38: 17, 293: 16, 363: 1, 37: 20, 58: 34, 318: 6, 331: 2, 283: 24, 17: 2, 50: 37, 19: 3, 325: 5, 308: 14, 328: 4, 13: 1, 315: 4, 32: 17, 23: 5, 326: 1, 312: 9, 311: 9, 12: 1, 330: 2, 324: 7, 14: 2, 317: 2, 314: 4, 316: 5, 329: 3, 16: 3, 337: 1, 320: 2, 421: 1, 336: 1, 5: 1, 18: 1, 345: 1, 319: 2, 458: 1, 323: 1, 327: 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10iAbGGuNAKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ec34ba3-e027-4633-87fd-c4f9d7b0dc29"
      },
      "source": [
        "for i in range(458):\n",
        "    print(count[457-i])\n",
        "    if i%100==0:\n",
        "        print()\n",
        "        print()\n",
        "# 140 must be reduce\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "\n",
            "\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "\n",
            "\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "3\n",
            "4\n",
            "1\n",
            "1\n",
            "5\n",
            "7\n",
            "1\n",
            "4\n",
            "5\n",
            "2\n",
            "2\n",
            "6\n",
            "2\n",
            "5\n",
            "4\n",
            "4\n",
            "12\n",
            "9\n",
            "9\n",
            "9\n",
            "11\n",
            "14\n",
            "9\n",
            "8\n",
            "21\n",
            "12\n",
            "22\n",
            "11\n",
            "22\n",
            "18\n",
            "17\n",
            "15\n",
            "18\n",
            "23\n",
            "18\n",
            "25\n",
            "16\n",
            "22\n",
            "18\n",
            "27\n",
            "29\n",
            "24\n",
            "30\n",
            "31\n",
            "24\n",
            "31\n",
            "24\n",
            "31\n",
            "28\n",
            "27\n",
            "32\n",
            "30\n",
            "33\n",
            "31\n",
            "25\n",
            "37\n",
            "39\n",
            "35\n",
            "42\n",
            "40\n",
            "47\n",
            "34\n",
            "36\n",
            "35\n",
            "42\n",
            "48\n",
            "45\n",
            "48\n",
            "38\n",
            "39\n",
            "41\n",
            "47\n",
            "46\n",
            "\n",
            "\n",
            "55\n",
            "47\n",
            "43\n",
            "49\n",
            "55\n",
            "30\n",
            "54\n",
            "57\n",
            "42\n",
            "45\n",
            "49\n",
            "62\n",
            "50\n",
            "48\n",
            "50\n",
            "46\n",
            "38\n",
            "58\n",
            "63\n",
            "62\n",
            "59\n",
            "50\n",
            "67\n",
            "60\n",
            "62\n",
            "59\n",
            "65\n",
            "62\n",
            "66\n",
            "74\n",
            "62\n",
            "72\n",
            "59\n",
            "50\n",
            "75\n",
            "83\n",
            "60\n",
            "79\n",
            "93\n",
            "74\n",
            "96\n",
            "89\n",
            "85\n",
            "91\n",
            "102\n",
            "89\n",
            "100\n",
            "87\n",
            "89\n",
            "78\n",
            "109\n",
            "100\n",
            "111\n",
            "78\n",
            "88\n",
            "89\n",
            "92\n",
            "101\n",
            "95\n",
            "98\n",
            "92\n",
            "100\n",
            "84\n",
            "100\n",
            "100\n",
            "101\n",
            "120\n",
            "123\n",
            "87\n",
            "98\n",
            "102\n",
            "96\n",
            "108\n",
            "108\n",
            "101\n",
            "104\n",
            "107\n",
            "101\n",
            "110\n",
            "125\n",
            "124\n",
            "133\n",
            "119\n",
            "111\n",
            "131\n",
            "130\n",
            "125\n",
            "129\n",
            "118\n",
            "125\n",
            "125\n",
            "143\n",
            "142\n",
            "108\n",
            "141\n",
            "136\n",
            "135\n",
            "140\n",
            "119\n",
            "150\n",
            "\n",
            "\n",
            "147\n",
            "130\n",
            "138\n",
            "161\n",
            "141\n",
            "139\n",
            "131\n",
            "127\n",
            "131\n",
            "115\n",
            "144\n",
            "139\n",
            "118\n",
            "138\n",
            "135\n",
            "136\n",
            "137\n",
            "119\n",
            "112\n",
            "112\n",
            "125\n",
            "124\n",
            "118\n",
            "116\n",
            "131\n",
            "122\n",
            "119\n",
            "116\n",
            "111\n",
            "106\n",
            "111\n",
            "113\n",
            "125\n",
            "143\n",
            "119\n",
            "124\n",
            "128\n",
            "132\n",
            "131\n",
            "119\n",
            "121\n",
            "89\n",
            "117\n",
            "122\n",
            "118\n",
            "105\n",
            "106\n",
            "108\n",
            "114\n",
            "106\n",
            "87\n",
            "104\n",
            "91\n",
            "95\n",
            "87\n",
            "93\n",
            "97\n",
            "101\n",
            "85\n",
            "68\n",
            "83\n",
            "90\n",
            "91\n",
            "76\n",
            "79\n",
            "67\n",
            "80\n",
            "71\n",
            "65\n",
            "77\n",
            "96\n",
            "79\n",
            "84\n",
            "69\n",
            "65\n",
            "77\n",
            "64\n",
            "63\n",
            "67\n",
            "67\n",
            "65\n",
            "71\n",
            "72\n",
            "60\n",
            "56\n",
            "53\n",
            "61\n",
            "52\n",
            "60\n",
            "55\n",
            "48\n",
            "51\n",
            "43\n",
            "48\n",
            "50\n",
            "53\n",
            "49\n",
            "41\n",
            "34\n",
            "44\n",
            "\n",
            "\n",
            "34\n",
            "42\n",
            "35\n",
            "22\n",
            "34\n",
            "29\n",
            "37\n",
            "35\n",
            "44\n",
            "33\n",
            "37\n",
            "31\n",
            "22\n",
            "26\n",
            "26\n",
            "26\n",
            "19\n",
            "18\n",
            "17\n",
            "20\n",
            "15\n",
            "30\n",
            "17\n",
            "15\n",
            "17\n",
            "21\n",
            "13\n",
            "11\n",
            "16\n",
            "11\n",
            "10\n",
            "13\n",
            "13\n",
            "5\n",
            "6\n",
            "5\n",
            "7\n",
            "3\n",
            "1\n",
            "2\n",
            "3\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyXSr2fR7qUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c866432a-7f87-4fa5-bd2f-14e6e404de95"
      },
      "source": [
        "#add padding from the starting, so list could be equal in size to 40\n",
        "maxlentweet = 350\n",
        "sequenceTrain = pad_sequences(sequenceTrain, maxlen=maxlentweet)\n",
        "print(sequenceTrain[40])\n",
        "print(sequenceTrain.shape)\n",
        "print(len(sequenceTrain))\n",
        "\n",
        "sequenceTest = pad_sequences(sequenceTest, maxlen=maxlentweet)\n",
        "print(sequenceTest[0])\n",
        "print(sequenceTest.shape)\n",
        "print(len(sequenceTest))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0  6174   895     3   653  2008   282    87   653  2008\n",
            " 13451     9    17   556   318   994   284   323    10     9  1605   784\n",
            "   181     5    58   446 12671    12    66   155   260   110   219    69\n",
            " 13451   287   466   994   615     8   358   141    28   321  6273   436\n",
            "  5140     3  6174   439   515   895   597    41  4864 33148     8 13451\n",
            "    14  3295     4   192   845     8     1   214     2  4570    23  1244\n",
            "    12 13451   994   615     7   113    68   672  1002   436     3   151\n",
            "   895   597     8 13451   293  2342  4415     1   669  3208     2 13451\n",
            "   994  1242     1   436     8 13451   994  6174  1373    12     4   159\n",
            "    18     1   317   318   895   558  3430  2088     2   994  4376     5\n",
            "   994  6174     3   878     1  2888   278    16     4   662   109    24\n",
            "   359   518     9    10     1 13451   278   287   513   436     2     4\n",
            "  1937  1098     2   994   615    17   864    10    45    77   240     5\n",
            "  1936     6   119   669  1067     6    24   152     7   122    10    24\n",
            "    56    15   192   137     3   446   920   466   994   651   436     3\n",
            "   895   597    11 12671    10    21    19    17    74     2   892   925\n",
            "    83  2452]\n",
            "(20972, 350)\n",
            "20972\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0   608   217  2044   673     6  1583  1358   170\n",
            "  1428     7    76   149 15731     2     1  1583  1358   624    23     4\n",
            "   679   170  1428    23     8  2485    22     7    30    10   624    21\n",
            "    19 13421   231     2     1   170    13  1004   394    47  1278   215\n",
            "     1   184     2     1   170    15   433  2044   673   184     6   169\n",
            "    13  3427     1   907     2    13   574     5    74  2154   325     2\n",
            "     1  1428   432   851    56   845     5  1130  8353  5184  2435  4797\n",
            "    16  1657   213     6   296    89     1  8761     2     1  1950   170\n",
            "   646     6  1728     5     4    47   751   748  2039  1879    52    11\n",
            "  4220   102]\n",
            "(8989, 350)\n",
            "8989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDJ_rOKg9omZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cd3dbd87-ba7c-4e2c-bf75-14d44dc62905"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open(\"/content/glove.6B.100d.txt\") as f:\n",
        "    for line in f.readlines():\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Total number of word vectors are:' , len(embeddings_index))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "max_words = len(embeddings_index)\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "count = 0\n",
        "for word, i in embeddings_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[count] = embedding_vector\n",
        "        count+=1\n",
        "\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of word vectors are: 400000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBUMwURW3on8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,SpatialDropout1D,GRU\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D,Conv1D,GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1NHwAkc3Xe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequenceTrain = np.array(sequenceTrain)\n",
        "computer = np.array(computer)\n",
        "maths = np.array(maths)\n",
        "physics = np.array(physics)\n",
        "stats = np.array(stats)\n",
        "bio = np.array(bio)\n",
        "finance = np.array(finance)\n",
        "\n",
        "# train_size = int(len(sequenceTrain) * 0.8)\n",
        "# train_x = sequenceTrain[:train_size]\n",
        "# train_y = computer[:train_size]\n",
        "# val_x = sequenceTrain[train_size:]\n",
        "# val_y = computer[train_size:]\n",
        "# print(train_x.shape , train_y.shape)\n",
        "# print(val_x.shape , val_y.shape)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ih7eT3GGJ1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6zAeS58-iHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "3b7fa039-162f-4ff2-b256-328f393f0be4"
      },
      "source": [
        "# max_words = 400000\n",
        "# embedding_dim = 100\n",
        "\n",
        "train_X = sequenceTrain\n",
        "train_Y = computer\n",
        "print(train_X.shape , train_Y.shape)\n",
        "\n",
        "\n",
        "modelC = tf.keras.Sequential([Input(shape=(maxlentweet,)),\n",
        "                             Embedding(max_words, embedding_dim, weights=[embedding_matrix]),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             GlobalMaxPool1D(),\n",
        "                             Dense(50, activation=\"relu\"),\n",
        "                             Dropout(0.2),\n",
        "                             Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "modelC.layers[0].trainable = False\n",
        "print(modelC.summary())\n",
        "\n",
        "modelC.compile(loss='binary_crossentropy' , optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3) ,metrics=['accuracy',get_f1])\n",
        "historyC = modelC.fit(train_X,\n",
        "                  train_Y,\n",
        "                  epochs = 3,\n",
        "                #   batch_size=2048,\n",
        "                  validation_split=0.1\n",
        "                #   validation_data=(val_x,val_y),\n",
        "                #   callbacks=[earlystopper,reduce_lr]\n",
        "                  )"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20972, 350) (20972,)\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 350, 100)          40000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_18 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "bidirectional_19 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_9 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 40,125,901\n",
            "Trainable params: 125,901\n",
            "Non-trainable params: 40,000,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "590/590 [==============================] - 37s 63ms/step - loss: 0.6360 - accuracy: 0.6392 - get_f1: 0.3242 - val_loss: 0.5803 - val_accuracy: 0.6945 - val_get_f1: 0.6931\n",
            "Epoch 2/3\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.5418 - accuracy: 0.7289 - get_f1: 0.6448 - val_loss: 0.5251 - val_accuracy: 0.7336 - val_get_f1: 0.6128\n",
            "Epoch 3/3\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.4873 - accuracy: 0.7672 - get_f1: 0.7086 - val_loss: 0.4601 - val_accuracy: 0.7836 - val_get_f1: 0.7704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDi1q0vx3r9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c175a260-f47f-4e20-cbff-06759e3121f9"
      },
      "source": [
        "# max_words = 400000\n",
        "# embedding_dim = 100\n",
        "\n",
        "train_X = sequenceTrain\n",
        "train_Y = physics\n",
        "print(train_X.shape , train_Y.shape)\n",
        "\n",
        "\n",
        "modelP = tf.keras.Sequential([Input(shape=(maxlentweet,)),\n",
        "                             Embedding(max_words, embedding_dim, weights=[embedding_matrix]),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             GlobalMaxPool1D(),\n",
        "                             Dense(50, activation=\"relu\"),\n",
        "                             Dropout(0.2),\n",
        "                             Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "\n",
        "modelP.layers[0].trainable = False\n",
        "print(modelP.summary())\n",
        "\n",
        "modelP.compile(loss='binary_crossentropy' , optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3) , metrics=['accuracy',get_f1])\n",
        "\n",
        "historyP = modelP.fit(train_X,\n",
        "                  train_Y,\n",
        "                  epochs = 12,\n",
        "                  validation_split=0.1\n",
        "                #   batch_size=2048,\n",
        "                #   validation_data=(val_x,val_y),\n",
        "                #   callbacks=[earlystopper,reduce_lr]\n",
        "                  )"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20972, 350) (20972,)\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 350, 100)          40000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_20 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "bidirectional_21 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_10 (Glo (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 40,125,901\n",
            "Trainable params: 125,901\n",
            "Non-trainable params: 40,000,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "590/590 [==============================] - 37s 63ms/step - loss: 0.5425 - accuracy: 0.7406 - get_f1: 0.2782 - val_loss: 0.4830 - val_accuracy: 0.7888 - val_get_f1: 0.4313\n",
            "Epoch 2/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.4277 - accuracy: 0.8157 - get_f1: 0.6084 - val_loss: 0.3813 - val_accuracy: 0.8475 - val_get_f1: 0.6576\n",
            "Epoch 3/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.3442 - accuracy: 0.8568 - get_f1: 0.7088 - val_loss: 0.3216 - val_accuracy: 0.8765 - val_get_f1: 0.7607\n",
            "Epoch 4/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2945 - accuracy: 0.8837 - get_f1: 0.7659 - val_loss: 0.2901 - val_accuracy: 0.8918 - val_get_f1: 0.7894\n",
            "Epoch 5/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2730 - accuracy: 0.8945 - get_f1: 0.7894 - val_loss: 0.2932 - val_accuracy: 0.8918 - val_get_f1: 0.7649\n",
            "Epoch 6/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2445 - accuracy: 0.9064 - get_f1: 0.8159 - val_loss: 0.2662 - val_accuracy: 0.9047 - val_get_f1: 0.8093\n",
            "Epoch 7/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2331 - accuracy: 0.9124 - get_f1: 0.8270 - val_loss: 0.2735 - val_accuracy: 0.8947 - val_get_f1: 0.7987\n",
            "Epoch 8/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2101 - accuracy: 0.9219 - get_f1: 0.8480 - val_loss: 0.2705 - val_accuracy: 0.9090 - val_get_f1: 0.8163\n",
            "Epoch 9/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2015 - accuracy: 0.9258 - get_f1: 0.8564 - val_loss: 0.2858 - val_accuracy: 0.8985 - val_get_f1: 0.7837\n",
            "Epoch 10/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1876 - accuracy: 0.9294 - get_f1: 0.8628 - val_loss: 0.2611 - val_accuracy: 0.9047 - val_get_f1: 0.8088\n",
            "Epoch 11/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1769 - accuracy: 0.9320 - get_f1: 0.8702 - val_loss: 0.2775 - val_accuracy: 0.9023 - val_get_f1: 0.8019\n",
            "Epoch 12/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1635 - accuracy: 0.9395 - get_f1: 0.8857 - val_loss: 0.2666 - val_accuracy: 0.9042 - val_get_f1: 0.8026\n",
            "Epoch 13/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1611 - accuracy: 0.9390 - get_f1: 0.8838 - val_loss: 0.2734 - val_accuracy: 0.9004 - val_get_f1: 0.8130\n",
            "Epoch 14/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1452 - accuracy: 0.9463 - get_f1: 0.8966 - val_loss: 0.2832 - val_accuracy: 0.9004 - val_get_f1: 0.8119\n",
            "Epoch 15/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1394 - accuracy: 0.9486 - get_f1: 0.9037 - val_loss: 0.2837 - val_accuracy: 0.9051 - val_get_f1: 0.8149\n",
            "Epoch 16/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1356 - accuracy: 0.9493 - get_f1: 0.9047 - val_loss: 0.2946 - val_accuracy: 0.8832 - val_get_f1: 0.7956\n",
            "Epoch 17/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1274 - accuracy: 0.9527 - get_f1: 0.9087 - val_loss: 0.2790 - val_accuracy: 0.9056 - val_get_f1: 0.8166\n",
            "Epoch 18/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1194 - accuracy: 0.9544 - get_f1: 0.9134 - val_loss: 0.2721 - val_accuracy: 0.9123 - val_get_f1: 0.8288\n",
            "Epoch 19/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1091 - accuracy: 0.9593 - get_f1: 0.9244 - val_loss: 0.3551 - val_accuracy: 0.9013 - val_get_f1: 0.7927\n",
            "Epoch 20/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1092 - accuracy: 0.9586 - get_f1: 0.9218 - val_loss: 0.3327 - val_accuracy: 0.9042 - val_get_f1: 0.8026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auvra_L_3sQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "aa9b43c8-94e6-403c-d023-3d5e46abb058"
      },
      "source": [
        "# max_words = 400000\n",
        "# embedding_dim = 100\n",
        "\n",
        "train_X = sequenceTrain\n",
        "train_Y = maths\n",
        "print(train_X.shape , train_Y.shape)\n",
        "\n",
        "\n",
        "modelM = tf.keras.Sequential([Input(shape=(maxlentweet,)),\n",
        "                             Embedding(max_words, embedding_dim, weights=[embedding_matrix]),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             GlobalMaxPool1D(),\n",
        "                             Dense(50, activation=\"relu\"),\n",
        "                             Dropout(0.2),\n",
        "                             Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "modelM.layers[0].trainable = False\n",
        "print(modelM.summary())\n",
        "\n",
        "modelM.compile(loss='binary_crossentropy' , optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3) , metrics=['accuracy',get_f1])\n",
        "\n",
        "historyM = modelM.fit(train_X,\n",
        "                  train_Y,\n",
        "                  epochs = 10,\n",
        "                  validation_split=0.1\n",
        "                #   batch_size=2048,\n",
        "                #   validation_data=(val_x,val_y),\n",
        "                #   callbacks=[earlystopper,reduce_lr]\n",
        "                  )"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20972, 350) (20972,)\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 350, 100)          40000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_22 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "bidirectional_23 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_11 (Glo (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 40,125,901\n",
            "Trainable params: 125,901\n",
            "Non-trainable params: 40,000,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "590/590 [==============================] - 37s 63ms/step - loss: 0.4927 - accuracy: 0.7819 - get_f1: 0.4167 - val_loss: 0.4036 - val_accuracy: 0.8294 - val_get_f1: 0.5794\n",
            "Epoch 2/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.4145 - accuracy: 0.8238 - get_f1: 0.5839 - val_loss: 0.3502 - val_accuracy: 0.8594 - val_get_f1: 0.6876\n",
            "Epoch 3/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.3669 - accuracy: 0.8509 - get_f1: 0.6670 - val_loss: 0.3462 - val_accuracy: 0.8599 - val_get_f1: 0.6595\n",
            "Epoch 4/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.3315 - accuracy: 0.8665 - get_f1: 0.7109 - val_loss: 0.2946 - val_accuracy: 0.8799 - val_get_f1: 0.7540\n",
            "Epoch 5/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.3078 - accuracy: 0.8750 - get_f1: 0.7332 - val_loss: 0.2892 - val_accuracy: 0.8851 - val_get_f1: 0.7531\n",
            "Epoch 6/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2794 - accuracy: 0.8883 - get_f1: 0.7596 - val_loss: 0.2992 - val_accuracy: 0.8737 - val_get_f1: 0.7696\n",
            "Epoch 7/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2685 - accuracy: 0.8918 - get_f1: 0.7758 - val_loss: 0.2670 - val_accuracy: 0.8942 - val_get_f1: 0.7744\n",
            "Epoch 8/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2483 - accuracy: 0.9015 - get_f1: 0.7952 - val_loss: 0.3286 - val_accuracy: 0.8613 - val_get_f1: 0.7571\n",
            "Epoch 9/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2405 - accuracy: 0.9030 - get_f1: 0.7989 - val_loss: 0.2660 - val_accuracy: 0.8899 - val_get_f1: 0.7774\n",
            "Epoch 10/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2244 - accuracy: 0.9101 - get_f1: 0.8114 - val_loss: 0.2669 - val_accuracy: 0.8961 - val_get_f1: 0.7934\n",
            "Epoch 11/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2125 - accuracy: 0.9148 - get_f1: 0.8237 - val_loss: 0.2755 - val_accuracy: 0.8870 - val_get_f1: 0.7848\n",
            "Epoch 12/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2003 - accuracy: 0.9203 - get_f1: 0.8345 - val_loss: 0.2794 - val_accuracy: 0.8913 - val_get_f1: 0.7830\n",
            "Epoch 13/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1929 - accuracy: 0.9227 - get_f1: 0.8405 - val_loss: 0.2868 - val_accuracy: 0.8789 - val_get_f1: 0.7799\n",
            "Epoch 14/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1834 - accuracy: 0.9277 - get_f1: 0.8541 - val_loss: 0.2756 - val_accuracy: 0.8813 - val_get_f1: 0.7774\n",
            "Epoch 15/15\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1777 - accuracy: 0.9305 - get_f1: 0.8584 - val_loss: 0.2692 - val_accuracy: 0.8866 - val_get_f1: 0.7798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOjSB79c3sb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7507b4d-b69b-4749-c058-d83081d45476"
      },
      "source": [
        "# max_words = 400000\n",
        "# embedding_dim = 100\n",
        "\n",
        "train_X = sequenceTrain\n",
        "train_Y = stats\n",
        "print(train_X.shape , train_Y.shape)\n",
        "\n",
        "modelS = tf.keras.Sequential([Input(shape=(maxlentweet,)),\n",
        "                             Embedding(max_words, embedding_dim, weights=[embedding_matrix]),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             GlobalMaxPool1D(),\n",
        "                             Dense(50, activation=\"relu\"),\n",
        "                             Dropout(0.2),\n",
        "                             Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "modelS.layers[0].trainable = False\n",
        "print(modelS.summary())\n",
        "\n",
        "modelS.compile(loss='binary_crossentropy' , optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)  , metrics=['accuracy',get_f1])\n",
        "\n",
        "historyS = modelS.fit(train_X,\n",
        "                  train_Y,\n",
        "                  epochs = 10,\n",
        "                  validation_split=0.1\n",
        "                #   batch_size=2048,\n",
        "                #   validation_data=(val_x,val_y),\n",
        "                #   callbacks=[earlystopper,reduce_lr]\n",
        "                  )"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20972, 350) (20972,)\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 350, 100)          40000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_24 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "bidirectional_25 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_12 (Glo (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 40,125,901\n",
            "Trainable params: 125,901\n",
            "Non-trainable params: 40,000,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "590/590 [==============================] - 37s 62ms/step - loss: 0.5508 - accuracy: 0.7527 - get_f1: 0.0274 - val_loss: 0.4651 - val_accuracy: 0.7664 - val_get_f1: 0.1360\n",
            "Epoch 2/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.4304 - accuracy: 0.7985 - get_f1: 0.4772 - val_loss: 0.3837 - val_accuracy: 0.8179 - val_get_f1: 0.5972\n",
            "Epoch 3/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.3735 - accuracy: 0.8284 - get_f1: 0.6198 - val_loss: 0.3844 - val_accuracy: 0.8136 - val_get_f1: 0.6825\n",
            "Epoch 4/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.3443 - accuracy: 0.8464 - get_f1: 0.6711 - val_loss: 0.3455 - val_accuracy: 0.8394 - val_get_f1: 0.7017\n",
            "Epoch 5/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.3212 - accuracy: 0.8564 - get_f1: 0.6901 - val_loss: 0.3251 - val_accuracy: 0.8394 - val_get_f1: 0.7015\n",
            "Epoch 6/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.3041 - accuracy: 0.8647 - get_f1: 0.7144 - val_loss: 0.3243 - val_accuracy: 0.8418 - val_get_f1: 0.6945\n",
            "Epoch 7/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2876 - accuracy: 0.8735 - get_f1: 0.7285 - val_loss: 0.3662 - val_accuracy: 0.8217 - val_get_f1: 0.6993\n",
            "Epoch 8/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2780 - accuracy: 0.8769 - get_f1: 0.7356 - val_loss: 0.3211 - val_accuracy: 0.8508 - val_get_f1: 0.7081\n",
            "Epoch 9/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2641 - accuracy: 0.8844 - get_f1: 0.7523 - val_loss: 0.3538 - val_accuracy: 0.8303 - val_get_f1: 0.7073\n",
            "Epoch 10/20\n",
            "590/590 [==============================] - 35s 60ms/step - loss: 0.2490 - accuracy: 0.8940 - get_f1: 0.7749 - val_loss: 0.3177 - val_accuracy: 0.8551 - val_get_f1: 0.7036\n",
            "Epoch 11/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2423 - accuracy: 0.8937 - get_f1: 0.7777 - val_loss: 0.3519 - val_accuracy: 0.8556 - val_get_f1: 0.6644\n",
            "Epoch 12/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2287 - accuracy: 0.9000 - get_f1: 0.7886 - val_loss: 0.3272 - val_accuracy: 0.8541 - val_get_f1: 0.7182\n",
            "Epoch 13/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2198 - accuracy: 0.9058 - get_f1: 0.7985 - val_loss: 0.3264 - val_accuracy: 0.8499 - val_get_f1: 0.7084\n",
            "Epoch 14/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.2083 - accuracy: 0.9113 - get_f1: 0.8140 - val_loss: 0.3911 - val_accuracy: 0.8251 - val_get_f1: 0.6994\n",
            "Epoch 15/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.2017 - accuracy: 0.9131 - get_f1: 0.8165 - val_loss: 0.3378 - val_accuracy: 0.8561 - val_get_f1: 0.7188\n",
            "Epoch 16/20\n",
            "590/590 [==============================] - 35s 60ms/step - loss: 0.1945 - accuracy: 0.9167 - get_f1: 0.8270 - val_loss: 0.3520 - val_accuracy: 0.8503 - val_get_f1: 0.7165\n",
            "Epoch 17/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1867 - accuracy: 0.9221 - get_f1: 0.8354 - val_loss: 0.3430 - val_accuracy: 0.8518 - val_get_f1: 0.7112\n",
            "Epoch 18/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1761 - accuracy: 0.9266 - get_f1: 0.8464 - val_loss: 0.3830 - val_accuracy: 0.8322 - val_get_f1: 0.7045\n",
            "Epoch 19/20\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1716 - accuracy: 0.9288 - get_f1: 0.8498 - val_loss: 0.3370 - val_accuracy: 0.8589 - val_get_f1: 0.7072\n",
            "Epoch 20/20\n",
            "590/590 [==============================] - 36s 60ms/step - loss: 0.1678 - accuracy: 0.9306 - get_f1: 0.8538 - val_loss: 0.3694 - val_accuracy: 0.8508 - val_get_f1: 0.6986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzDGwhf23ssm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "8c3474d0-d037-4059-d857-7c0889154a41"
      },
      "source": [
        "# max_words = 400000\n",
        "# embedding_dim = 100\n",
        "\n",
        "train_X = sequenceTrain\n",
        "train_Y = bio\n",
        "print(train_X.shape , train_Y.shape)\n",
        "\n",
        "modelB = tf.keras.Sequential([Input(shape=(maxlentweet,)),\n",
        "                             Embedding(max_words, embedding_dim, weights=[embedding_matrix]),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             GlobalMaxPool1D(),\n",
        "                             Dense(50, activation=\"relu\"),\n",
        "                             Dropout(0.2),\n",
        "                             Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "modelB.layers[0].trainable = False\n",
        "print(modelB.summary())\n",
        "\n",
        "modelB.compile(loss='binary_crossentropy' , optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)  , metrics=['accuracy',get_f1])\n",
        "\n",
        "historyB = modelB.fit(train_X,\n",
        "                  train_Y,\n",
        "                  epochs = 5,\n",
        "                  validation_split=0.1\n",
        "                #   batch_size=2048,\n",
        "                #   validation_data=(val_x,val_y),\n",
        "                #   callbacks=[earlystopper,reduce_lr]\n",
        "                  )"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20972, 350) (20972,)\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 350, 100)          40000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_26 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "bidirectional_27 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_13 (Glo (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 40,125,901\n",
            "Trainable params: 125,901\n",
            "Non-trainable params: 40,000,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "590/590 [==============================] - 37s 63ms/step - loss: 0.1388 - accuracy: 0.9699 - get_f1: 1.9940e-04 - val_loss: 0.1065 - val_accuracy: 0.9776 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1338 - accuracy: 0.9714 - get_f1: 0.0000e+00 - val_loss: 0.1050 - val_accuracy: 0.9776 - val_get_f1: 0.0000e+00\n",
            "Epoch 3/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1273 - accuracy: 0.9714 - get_f1: 0.0000e+00 - val_loss: 0.1078 - val_accuracy: 0.9776 - val_get_f1: 0.0000e+00\n",
            "Epoch 4/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1219 - accuracy: 0.9714 - get_f1: 0.0000e+00 - val_loss: 0.0990 - val_accuracy: 0.9776 - val_get_f1: 0.0000e+00\n",
            "Epoch 5/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.1102 - accuracy: 0.9711 - get_f1: 0.0017 - val_loss: 0.0921 - val_accuracy: 0.9776 - val_get_f1: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGDLFMFm-qKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "6276b8f6-191f-4605-8dd3-b6b3b6968917"
      },
      "source": [
        "# max_words = 400000\n",
        "# embedding_dim = 100\n",
        "\n",
        "train_X = sequenceTrain\n",
        "train_Y = finance\n",
        "print(train_X.shape , train_Y.shape)\n",
        "\n",
        "modelF = tf.keras.Sequential([Input(shape=(maxlentweet,)),\n",
        "                             Embedding(max_words, embedding_dim, weights=[embedding_matrix]),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2)),\n",
        "                             GlobalMaxPool1D(),\n",
        "                             Dense(50, activation=\"relu\"),\n",
        "                             Dropout(0.2),\n",
        "                             Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "modelF.layers[0].trainable = False\n",
        "print(modelF.summary())\n",
        "\n",
        "modelF.compile(loss='binary_crossentropy' , optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)  , metrics=['accuracy',get_f1])\n",
        "\n",
        "historyF = modelF.fit(train_X,\n",
        "                  train_Y,\n",
        "                  epochs = 5,\n",
        "                  validation_split=0.1\n",
        "                #   batch_size=2048,\n",
        "                #   validation_data=(val_x,val_y),\n",
        "                #   callbacks=[earlystopper,reduce_lr]\n",
        "                  )"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20972, 350) (20972,)\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 350, 100)          40000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_28 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "bidirectional_29 (Bidirectio (None, 350, 100)          60400     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_14 (Glo (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 40,125,901\n",
            "Trainable params: 125,901\n",
            "Non-trainable params: 40,000,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "590/590 [==============================] - 37s 63ms/step - loss: 0.0695 - accuracy: 0.9883 - get_f1: 0.0000e+00 - val_loss: 0.0736 - val_accuracy: 0.9862 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.0659 - accuracy: 0.9883 - get_f1: 0.0000e+00 - val_loss: 0.0721 - val_accuracy: 0.9862 - val_get_f1: 0.0000e+00\n",
            "Epoch 3/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.0646 - accuracy: 0.9883 - get_f1: 0.0000e+00 - val_loss: 0.0722 - val_accuracy: 0.9862 - val_get_f1: 0.0000e+00\n",
            "Epoch 4/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.0614 - accuracy: 0.9883 - get_f1: 0.0000e+00 - val_loss: 0.0688 - val_accuracy: 0.9862 - val_get_f1: 0.0000e+00\n",
            "Epoch 5/5\n",
            "590/590 [==============================] - 36s 61ms/step - loss: 0.0526 - accuracy: 0.9884 - get_f1: 0.0040 - val_loss: 0.0506 - val_accuracy: 0.9867 - val_get_f1: 0.0152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9XCSfHfEBov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = []\n",
        "val = 0.4\n",
        "for ss in modelC.predict(sequenceTest):\n",
        "    if ss>val:\n",
        "        C.append(1)\n",
        "    else:\n",
        "        C.append(0)\n",
        "\n",
        "\n",
        "P = []\n",
        "for ss in modelP.predict(sequenceTest):\n",
        "    if ss>val:\n",
        "        P.append(1)\n",
        "    else:\n",
        "        P.append(0)\n",
        "\n",
        "\n",
        "M = []\n",
        "for ss in modelM.predict(sequenceTest):\n",
        "    if ss>val:\n",
        "        M.append(1)\n",
        "    else:\n",
        "        M.append(0)\n",
        "\n",
        "\n",
        "S = []\n",
        "for ss in modelS.predict(sequenceTest):\n",
        "    if ss>val:\n",
        "        S.append(1)\n",
        "    else:\n",
        "        S.append(0)\n",
        "\n",
        "\n",
        "B = []\n",
        "for ss in modelB.predict(sequenceTest):\n",
        "    if ss>val:\n",
        "        B.append(1)\n",
        "    else:\n",
        "        B.append(0)\n",
        "\n",
        "F = []\n",
        "for ss in modelF.predict(sequenceTest):\n",
        "    if ss>val:\n",
        "        F.append(1)\n",
        "    else:\n",
        "        F.append(0)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prbrubH9-qH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "debf41e5-f53d-4e97-ac52-183d3c59dbae"
      },
      "source": [
        "dic = {'ID': colID, 'Computer Science': C , 'Physics':P, 'Mathematics':M, 'Statistics':S, 'Quantitative Biology':B, 'Quantitative Finance':F} \n",
        "df_fin = pd.DataFrame(dic) \n",
        "filename = \"sub.csv\"\n",
        "df_fin.to_csv(filename , index=False)\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e8f11b21-7e6b-4aad-abcd-eb14a741135b\", \"sub.csv\", 161895)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtDj1dYx-qFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4057c169-2352-40cb-bc30-3c39a4fe864d"
      },
      "source": [
        "# tt = pd.read_csv(\"/content/test.csv\")\n",
        "# col1 = []\n",
        "# for i in range(len(sequenceTest)):\n",
        "#     col1.append(tt[\"ID\"][i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-u4dPMklQ7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "67fb35a3-29c6-4229-e7fe-821d00602a38"
      },
      "source": [
        "print(sequenceTest.shape)\n",
        "print(sequenceTest[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8989, 300)\n",
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deDZPnnK-qD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "43564b1f-8d95-4abe-db89-f5c2fa96465b"
      },
      "source": [
        "ind = 2\n",
        "print(modelC.predict(sequenceTest[ind].reshape(1,300)))\n",
        "print(modelP.predict(sequenceTest[ind].reshape(1,300)))\n",
        "print(modelM.predict(sequenceTest[ind].reshape(1,300)))\n",
        "print(modelS.predict(sequenceTest[ind].reshape(1,300)))\n",
        "print(modelB.predict(sequenceTest[ind].reshape(1,300)))\n",
        "print(modelF.predict(sequenceTest[ind].reshape(1,300)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.3899062]]\n",
            "[[0.56794864]]\n",
            "[[0.51846033]]\n",
            "[[0.21701778]]\n",
            "[[0.596456]]\n",
            "[[0.6026816]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEjDrMBc-p91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YX09Ao-p4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bvVVlBhpTz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(X , Y):\n",
        "    # classifier = LGBMClassifier()\n",
        "    # param_grid = {\"max_depth\":[4,6,8,10,-1],\n",
        "    #       \"num_leaves\":[7,15,31,63,127],\n",
        "    #       \"min_data_in_leaf\":[1,3,5,7,10,25],\n",
        "    #       \"n_estimators\":list(range(50,1001,100))}\n",
        "\n",
        "    # model = RandomizedSearchCV(estimator = classifier,\n",
        "    #                     param_distributions = param_grid,\n",
        "    #                     cv = 5,\n",
        "    #                     n_iter = 10,\n",
        "    #                     verbose=1000\n",
        "    #                     )\n",
        "\n",
        "    # classifier = XGBClassifier(n_jobs = -1)\n",
        "    # param_grid = {\n",
        "    #     \"n_estimators\" : np.arange(100,1200,100),\n",
        "    #     \"max_depth\" : np.arange(3,10)\n",
        "    # }\n",
        "\n",
        "    # model = RandomizedSearchCV(estimator = classifier,\n",
        "    #                     param_distributions = param_grid,\n",
        "    #                     cv = 5,\n",
        "    #                     n_iter = 10,\n",
        "    #                     verbose=1000\n",
        "    #                     )\n",
        "\n",
        "    # classifier = CatBoostClassifier(verbose=0)\n",
        "    # param_grid = {\n",
        "    #     \"n_estimators\" : np.arange(100,1200,100),\n",
        "    #     \"depth\" : np.arange(3,10)\n",
        "    # }\n",
        "\n",
        "    # model = RandomizedSearchCV(estimator = classifier,\n",
        "    #                     param_distributions = param_grid,\n",
        "    #                     cv = 5,\n",
        "    #                     n_iter = 10,\n",
        "    #                     verbose=1000\n",
        "    #                     )\n",
        "\n",
        "    model = CatBoostClassifier(n_estimators=1100, depth=3, verbose=0)\n",
        "    # model = RandomForestClassifier()\n",
        "    # model = XGBClassifier(n_estimators=800 , max_depth=5 , verbose=10)\n",
        "    # model = LGBMClassifier(max_depth=30 , n_estimators=500 , num_leaves=31)\n",
        "    model.fit(X , Y)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def prediction(model):\n",
        "    drop = copy.deepcopy(col_to_drop)\n",
        "    X = test.drop(drop , axis=1)\n",
        "    print(X.shape)\n",
        "\n",
        "    col = model.predict_proba(X)\n",
        "    # col = model.predict_log_proba(X)\n",
        "\n",
        "    return col\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj8DIwNoU3bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2f63ba6-01ac-4b7d-956e-7614bc6f507c"
      },
      "source": [
        "# Offered_By\tCategory\tRating\tReviews\tSize\tPrice\tContent_Rating\tLast_Updated_On\tRelease_Version\tOS_Version_Required\tDownloads\ttrain_flag\n",
        "col_to_drop = [\"Offered_By\", \"Last_Updated_On\", \"train_flag\", \"Downloads\"]\n",
        "\n",
        "\n",
        "\n",
        "X = train.drop(col_to_drop , axis=1)\n",
        "Y = train[\"Downloads\"]\n",
        "print(X.shape, Y.shape)\n",
        "m1 = train_model(X , Y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16516, 8) (16516,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aHQiNk0phRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4272a424-d5de-4c01-cef7-cb2ef0e28b48"
      },
      "source": [
        "print(m1.score(X,Y))\n",
        "col = prediction(m1)\n",
        "print(col.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# col4 = []\n",
        "# if col3.shape == (len(df_test),1):\n",
        "#     for i in range(len(df_test)):\n",
        "#         col4.append(col3[i][0])\n",
        "#     col3 = copy.deepcopy(col4)\n",
        "# print(np.array(col3).shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6078348268345847\n",
            "(24776, 8)\n",
            "(24776, 18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipYc0m0Yw9nO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HwTYMF0puxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9B0-xlzrLz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggAHmZVrL4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDoRGdKdrL-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8JaKcF6rMC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz3rVmo2rMFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPhNen0nrMJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8HC8CK-rL88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLgrR29ErL3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nel0po4CrLyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmHU_kk7Hv20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a1 = pd.read_csv(\"/content/lgbt1.csv\")\n",
        "a2 = pd.read_csv(\"/content/lgbt2.csv\")\n",
        "b1 = pd.read_csv(\"/content/xg1.csv\")\n",
        "b2 = pd.read_csv(\"/content/xg2.csv\")\n",
        "c1 = pd.read_csv(\"/content/ran1.csv\")\n",
        "c2 = pd.read_csv(\"/content/ran2.csv\")\n",
        "d1 = pd.read_csv(\"/content/cat1.csv\")\n",
        "d2 = pd.read_csv(\"/content/cat2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-5qkl4IxXU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c4dd316-0a08-49e1-956b-edb3457635b7"
      },
      "source": [
        "a1[\"breed_category\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltw4TipfHv0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "90f82ec7-0ba9-4066-db7d-ebdc4412dbc0"
      },
      "source": [
        "col1 , col2 , col3 = [] , [] , []\n",
        "for i in range(len(a1)):\n",
        "    arg1 = a1[\"breed_category\"][i]\n",
        "    arg2 = a2[\"breed_category\"][i]\n",
        "    arg3 = b1[\"breed_category\"][i]\n",
        "    arg4 = b2[\"breed_category\"][i]\n",
        "    arg5 = c1[\"breed_category\"][i]\n",
        "    arg6 = c2[\"breed_category\"][i]\n",
        "    arg7 = d1[\"breed_category\"][i]\n",
        "    arg8 = d2[\"breed_category\"][i]\n",
        "    # lis1 = [arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8]\n",
        "    lis1 = [ arg2, arg4 , arg8]\n",
        "    ans1 = max(set(lis1), key = lis1.count) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    col1.append(a1[\"pet_id\"][i])\n",
        "    col2.append(ans1)\n",
        "    col3.append(ans2)\n",
        "\n",
        "\n",
        "dict = {'pet_id': col1, 'breed_category': col2 , 'pet_category':col3} \n",
        "df_fin = pd.DataFrame(dict) \n",
        "filename = \"qqqqqqqqqqqqq.csv\"\n",
        "df_fin.to_csv(filename , index=False)\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cfd46f20-9efb-4637-9ab4-b80338f88588\", \"qqqqqqqqqqqqq.csv\", 137259)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcqJGYF-WHZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkMTUGveWHSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2MP8w_HGDpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CjqjyGGFcHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CKVw6SwTY_Q",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}